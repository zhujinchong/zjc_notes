[DPO——RLHF 的替代之《Direct Preference Optimization: Your Language Model is Secretly a Reward Model》论文阅读 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/634705904)

[

UCLA华人提出全新自我对弈机制，LLM自己训自己，效果碾压GPT-4专家指导-36氪 (36kr.com)](https://www.36kr.com/p/2632885145549960)





[OpenAI：Superalignment的一种途径——Weak-to-Strong Generalization - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/685280689)





# DoRA

https://zhuanlan.zhihu.com/p/682556551



# LISA



https://mp.weixin.qq.com/s/Y_Xbpaat3ClZ6LIUkUzg9A