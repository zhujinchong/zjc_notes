# DoRA

https://zhuanlan.zhihu.com/p/682556551

LoRA原理：

![image-20240624151532615](images/11DPO_Lora/image-20240624151532615.png)

LoRA（Low Rank Adaptation，低秩适应）有一个潜在的继承者，称为 DoRA。DoRA可以看作是建立在LoRA之上的改进或扩展。DoRA是最近一篇论文《DoRA: Weight-Decomposed Low-Rank Adaptation》（《DoRA：权重分解的低秩适应》）提出的。

DoRA 方法首先将预训练的权重矩阵分解为幅度向量 （m） 和方向矩阵 （V）。然后，它采用方向矩阵 V 并对其应用标准的LoRA，即：

![image-20240624151555843](images/11DPO_Lora/image-20240624151555843.png)

开发这种方法的动机是基于分析和比较 LoRA 和全面微调模式的区别。论文发现，LoRA可以按比例增加或减少幅度和方向更新，但似乎缺乏像完全微调那样仅进行细微的方向变化的能力。因此，研究人员提出了幅度和方向分量的解耦。换句话说，他们的 **DoRA 方法旨在仅将 LoRA 应用于方向分量（同时还允许单独训练幅度分量）**。

请注意，在 DoRA 中引入幅度向量 m 比标准 LoRA 多添加 0.01% 的参数。然而，在LLM和视觉转换器基准测试中，他们发现，如果DoRA等级减半，即**当DoRA仅使用常规LoRA的一半参数时，DoRA的性能甚至优于LoRA**。

![image-20240624151824591](images/11DPO_Lora/image-20240624151824591.png)



# DPO

https://www.cnblogs.com/lemonzhang/p/17910358.html

在之前，我们已经了解到基于人类反馈的强化学习[RLHF分为三个阶段](https://www.cnblogs.com/lemonzhang/p/17819158.html)：全监督微调（SFT）、奖励模型（RM）、强化学习（PPO）。但是RLHF面临缺陷：**RLHF 是一个复杂且经常不稳定的过程**，首先拟合反映人类偏好的奖励模型，然后使用强化学习微调大型无监督 LM，以最大化这种估计奖励，而不会偏离原始模型太远。为解决这一问题，**提出一个直接偏好优化 (DPO) 的新算法：通过利用奖励函数与最优策略之间的映射关系，证明这个受限的奖励最大化问题可以通过单阶段的策略训练来精确优化，本质上是在人类偏好数据上解决一个分类问题**。DPO是稳定的、性能和计算成本轻量级的，无需拟合奖励模型，在微调期间从 LM 中采样，或执行显着的超参数调整。通过实验表明：DPO 进行微调超过了 RLHF 效果，并提高了摘要和单轮对话的响应质量。

![img](images/11DPO_Lora/1070495-20240115092404903-311542597.png)

与RLHF不同，DPO不依赖于明确的奖励建模或强化学习。它针对与RLHF相同的目标，但提供了一种更简单、更直接的培训方法。

**DPO的工作原理：增加偏好样本的对数概率与减小非偏好样本响应的对数概率。它结合了动态加权机制，以避免仅使用概率比目标时遇到的模型退化问题。**

DPO依赖于理论上的偏好模型，如Bradley-Terry模型，来测量奖励函数与经验偏好数据的对齐程度。**与传统方法不同，传统方法使用偏好模型来训练奖励模型，然后基于该奖励模型训练策略，DPO直接根据策略定义偏好损失。给定一个关于模型响应的人类偏好数据集，DPO可以使用简单的二元交叉熵目标来优化策略，无需在训练过程中明确学习奖励函数或从策略中采样。**

**（1）原RLHF的优化目标**：最大化奖励和最小化参考策略的KL散度

![img](images/11DPO_Lora/1070495-20240115093625510-972938794.png)

**（2）DPO优化目标：**利用了从奖励函数到最优策略的解析映射，允许直接使用人类偏好数据进行简化的优化过程

![img](images/11DPO_Lora/1070495-20240115092911186-835855853.png)

该目标增加了对偏好数据 $y_w$ 的可能性，并减少了非偏好数据 $y_l$ 的可能性。这些示例按照隐式奖励模型的评级加权，由 $\beta$ 缩放.



步骤：

步骤1）是在构造数据集，通过对同一问题的两种回复的倾向性：chosen or rejected，反映人类偏好。

步骤2）在于优化，具体过程大概是，对于同一个question prompt，模型在两种模型：language/policy model 和 reference model下分别生成，对应chosen 和 rejected label真值标签的生成概率，因此可以获得四种概率值：policy_chosen_logps, policy_rejected_logps, reference_chosen_logps, reference_rejected_logps, 用于DPO loss计算。





