

# 基本概念

## 基本概念

![e8fc39f9f7f291dc8b121858a201545b.png](images/e8fc39f9f7f291dc8b121858a201545b.png)



state：s

action: a

policy: $\pi$

reward: R

policy funciton: 

![image-20240629115515229](images/image-20240629115515229.png)

state transition: 

![image-20240629115600540](images/image-20240629115600540.png)

trajectory轨迹: 

![image-20240629115352363](images/image-20240629115352363.png)

return：(aka, cumulative future retward)

![image-20240629115832749](images/image-20240629115832749.png)

Discounted return: (aka cumulative discounted future reward)

![image-20240629115945313](images/image-20240629115945313.png)

> 因为s和a有随机性，所以 $R_{t+n}$ 和 $U_t$ 也有随机性，所以一般用期望 $E[U_t]$ 消除随机性。



## 重要概念

**重要概念：Action-value function:**

动作价值函数：

![image-20240629120109627](images/image-20240629120109627.png)

动作价值函数的意义：

* 如果已知 $\pi$ ，那么 $Q_{\pi}$ 可以评估当前动作 a 

提出问题：

* 如何求 $\pi$ ？

需要解决：

*  $\pi$ 需要每次都选择最好的动作，因此可以最大化 $Q_{\pi}$ ，即 Optimal action-value function.

Optimal action-value function:

![image-20240629121330738](images/image-20240629121330738.png)

**重要概念：State-value function：**

状态价值函数：

![image-20240629120857995](images/image-20240629120857995.png)

状态价值函数的意义：

* 如果已知 $\pi$ ，那么 $V_{\pi}$ 可以评估当前状态 s；
* 用 $E_s[V_{\pi}(s)]$ 评估 $\pi$ 的好坏



## 如何实现Agent

![image-20230707212954313](images/image-20230707212954313.png)



policy-based learning:

* 用Polity network 近似  $\pi$ 

value-based learning:

* 用DQN(Deep Q network) 近似 $Q^{\star}$ （因为有了Q网络，也可以指导当前动作）



## Value-Based

基于Q函数的DQN算法，也就是Q-Learning、Deep Q-Learning.

已知 $Q^{\star}$ 的定义：

![image-20240629153846447](images/image-20240629153846447.png)

首先用神经网络 ![image-20240629153310218](images/image-20240629153310218.png) 表示 $Q^{\star}$ 函数：（这里有两种网络形状，都可以表示）

![image-20230715094115804](images/image-20230715094115804.png)

使用TD算法迭代：

![image-20240629153639450](images/image-20240629153639450.png)

TD算法示意图：

![image-20230715222328115](images/image-20230715222328115.png)

算法描述：

![image-20240629154011718](images/image-20240629154011718.png)

这里提出疑问：

* 奖励r如何定义？

个人理解：

* 奖励r类似于标签(ground truth)，可能是训练数据吧。



## Policy-Based

已知，状态价值函数：

![image-20240629161029623](images/image-20240629161029623.png)

用神经网络表示 $\pi$ 策略函数，定义策略神经网络：

![image-20240629161219444](images/image-20240629161219444.png)

为了得到最优的$\pi$，可以最大化状态价值函数V：

![image-20240629161509731](images/image-20240629161509731.png)

![image-20240629161744265](images/image-20240629161744265.png)

经过推导Policy gradient，得到：

![image-20240629162924745](images/image-20240629162924745.png)

现实中A可能是连续的，或者很多值。此时需要采样计算（下面算法是采样一次）。

![image-20240629163651920](images/image-20240629163651920.png)

其中Q函数是不知道的，先看算法流程：

![image-20240629163828160](images/image-20240629163828160.png)

如何计算Q？

1、蒙特卡罗采样：

![image-20240629191507121](images/image-20240629191507121.png)

2、用神经网络近似Q：这就是Actor-Critic方法。



## Actor-Critic

已知，状态价值函数：

![image-20240629161029623](images/image-20240629161029623.png)

* 用神经网络![image-20240629213506774](images/image-20240629213506774.png) 表示 $\pi$ 函数
* 用神经网络 ![image-20240629153310218](images/image-20240629153310218.png) 表示 $Q$ 函数

得到：

![image-20240629213631946](images/image-20240629213631946.png)

算法流程：

![image-20240629213801903](images/image-20240629213801903.png)

其中第4和第5步是关键，展开如下：

![image-20240629214302736](images/image-20240629214302736.png)

总结算法流程如下：

![image-20240629214343010](images/image-20240629214343010.png)

注意：第9步的$q_t$在论文里用$\delta_t$ ，可以得到更好的结果。（有详细推导）



# AlphaGo

2016年，AlphaGo打败李世石。

强化学习和模仿学习（Imitation learning, behavior cloning）的区别：

* 强化学习有奖励函数，模仿学习没有奖励函数；
* 模仿学习是有监督学习。



**环境：**

在围棋世界，状态就是0和1组成19x19x2张量，分别表示黑棋和白起落子位置。动作就是361个可能落子的位置。AlphaGo 中用了19x19x48张量，为什么是48未知。AlphaGo Zero用了19x19x17张量，其中19x19x8是黑子最近8步的状态，19x19x8个是白子最近8步的状态，19x19x1个用全0或全1表示该黑子或白子行棋。



**AlphaGo的训练过程有3步：**

1、用behavior cloning训练一个policy network（监督学习）；

2、用policy gradient接着训练policy network；

3、用policy network训练一个value network.（因为这两个是分开训练的，所以AlphaGo不是Actor-Critic策略）



**AlphaGo的使用有两种：**

1、只用policy network每次生成一个最优步，但是效果不稳定。

2、利用value network，使用蒙特卡洛树搜索，比较稳定。



问：为什么要用policy gradient接着训练策略网络？

答：监督学习泛化效果不好，对未见过的状态处理不好。

数据显示：Behavior cloning + RL beats behavior cloning  with 80% chance.



## behavior cloning

只能打败高等业务爱好者，不能打败高手。

![image-20240630104744740](images/image-20240630104744740.png)



## policy network

定义奖励函数：

![image-20240629232007209](images/image-20240629232007209.png)

同样，Q函数我们不知道，可以用U代替，那么近似policy gradient如下：

![image-20240629232208131](images/image-20240629232208131.png)

训练方法如下：

![image-20240630104131179](images/image-20240630104131179.png)



## value network

在前面Value-Based章节我们定义的是Q网络，但是这里我们定义的是V网络

![image-20240630105210765](images/image-20240630105210765.png)

训练步骤如下：

![image-20240630105236582](images/image-20240630105236582.png)



## Monte Carlo Tree Search

蒙特卡洛树搜索先进行搜索+打分，即选出一个动作并进行打分，步骤如下：

**1、Selection 选择动作**

选出一个分数最高的动作，分数计算方式如下：（Q分数在第4步定义）

![image-20240630112549165](images/image-20240630112549165.png)



**2、Expansion 扩展棋局**

模拟对手下棋：利用 $\pi$ 网络，随机采样一个动作。

是的，这里是根据概率采样，因为对手可能下任意点。

![image-20240630110645032](images/image-20240630110645032.png)



**3、Evaluation 评估** $s_{t+1}$ 

* 利用 $\pi$ 网络，一直对弈到最后，得到奖励+1或-1.

* 利用V网络直接评估

将上面两种评估分数取平均值，记为：

![image-20240630111214067](images/image-20240630111214067.png)

因为对弈到最后，所以会得到一系列V值：

![image-20240630111620350](images/image-20240630111620350.png)



**4、Backup 回溯** $a_t$ 

利用上一步的到一系列V值，计算 $a_t$ 动作的分数：

![image-20240630111808582](images/image-20240630111808582.png)

![image-20240630111846371](images/image-20240630111846371.png)



在最后，MCTS做决策并不是用Q，而是用N：

![image-20240630114433583](images/image-20240630114433583.png)

![image-20240630114444494](images/image-20240630114444494.png)

## AlphaGo Zero

2017年，AlphaZero全胜AlphaGo

AlphaZero做了如下改进：

* AlphaGo Zero没有使用Behavior cloning. （说明人的经验不是全对的）
* 训练policy network时使用了MCTS.

![image-20240630115212899](images/image-20240630115212899.png)



# Value-Based中TD算法

时序差分算法

## Sarsa & Q-Learning

Sarsa和Q-Learning的区别仅在于一个用Q函数，一个用$Q^{\star}$ 函数：

![image-20240630134634244](images/image-20240630134634244.png)

Sarsa: （下一个动作是采样得到的）

![image-20240630134746043](images/image-20240630134746043.png)

Q-Learning:（下一个动作是取最大值得到的）

![image-20240630134715789](images/image-20240630134715789.png)

## Multi-Step TD Target

前面讲的TD都是一步，已知：

![image-20240701205622548](images/image-20240701205622548.png)

所以，

m步的Sarsa算法是：

![image-20240701205713118](images/image-20240701205713118.png)

m步的Q-Learning算法是：

![image-20240701205738845](images/image-20240701205738845.png)



# Value-Based高阶技巧

## Experience Replay

在Value-Based中，我们已知TD算法：

![image-20240629154011718](images/image-20240629154011718.png)

在这里，我们假设：

![image-20240701213229710](images/image-20240701213229710.png)



在算法中，我们使用一个transition：![image-20240701212454318](images/image-20240701212454318.png) 算出来的 $\delta_t$ 更新网络，有两个地方可以优化：

1、一个transition![image-20240701212454318](images/image-20240701212454318.png) 应该训练几次？应该如何采样？

2、面对不均匀的s状态（打小怪场景多，打boss场景少），不能采用均匀采样，应该采用importance samping.



第一个问题解决：

![image-20240701212454318](images/image-20240701212454318.png) 应该存到 replay buffer里面，容量是一个超参数。每次 randomly sample，因为打乱训练效果更好。



第二问题解决：

使用**Prioritized Experience Replay**，如果 $\delta_t$ （TD误差）比较大，说明数据重要，采样概率更高，学习率更大。

![image-20240701214222656](images/image-20240701214222656.png)



## 高估问题

TD训练DQN时，TD target是被高估的，因为：

![image-20240701220040016](images/image-20240701220040016.png)

TD target是被高估 -> 导致SGD被高估 -> 导致最终模型被高估。

又因为transition中的action通常是不均匀 -> 导致模型训练不均匀 -> 导致动作被高估的也不均匀 -> 导致模型不精准。

总结，两个原因：

1、TD Target被高估；

2、bootstraping方式训练的模型。（即，左脚踩右脚训练）



解决办法：Double DQN

1、训练一个target network，网络结构和参数可以定期从DQN网络复制。

2、选择动作还是从DQN，但是计算y值用target network.

![image-20240701221813981](images/image-20240701221813981.png)



## Dueling Network

2016年 Dueling Network得到更好的结果，我们来看下怎么实现的。

定义A函数（Optimal advantage function）：

![image-20240701223628849](images/image-20240701223628849.png)

已知：

![image-20240701223830055](images/image-20240701223830055.png)

那么：

![image-20240701223851610](images/image-20240701223851610.png)

那么，就有公式2：

![image-20240701224157331](images/image-20240701224157331.png)

用DQN的方法，训练一个A网络和一个V网络，结合起来就是Q网络。

![image-20240701224330018](images/image-20240701224330018.png)

效果很好。

为什么公式最后要减去max A?

![image-20240701224457309](images/image-20240701224457309.png)



# Policy-Based中Baseline

## 定义Baseline

推导过程：

![image-20240702233608014](images/image-20240702233608014.png)

得出定理：

![image-20240702233553262](images/image-20240702233553262.png)

根据定理，得到新的Policy gradient:

![image-20240702233814429](images/image-20240702233814429.png)

通常MC采样得到a，可以近似估计$E_A$ 。然后进行梯度上升：

![image-20240702234220405](images/image-20240702234220405.png)

![image-20240702234238999](images/image-20240702234238999.png)

在这里的b有以下问题：

1、b影响g，影响梯度；

2、好的b会减小方差加速收敛。

什么是好的b？ V函数可以当b，并且V非常接近Q函数。（后面两节都在用baseline）

![image-20240702234738548](images/image-20240702234738548.png)

## REINFORCE with Baseline

上节我们知道：

![image-20240703000258480](images/image-20240703000258480.png)

先在我们要构建两个网络：

* $\pi(A_t | s_t; \theta)$ ：表示策略网络
* $V_{\pi}(s_t)$ : 表示价值网络。

注意，第一章中A2C是Q网络，这里是V网络，V网络不包含动作，V网络更有价值，因为：

![image-20240703000834016](images/image-20240703000834016.png)

两个网络可以共享参数：

![image-20240703000924394](images/image-20240703000924394.png)

reinforce算法：

![image-20240703003151024](images/image-20240703003151024.png)

根据算法总结：

* V网络和之前Q网络更新策略类似，V网络的误差类似于TD error.
* $\pi$网络之前仅用u更新，即，MC采样。
* $\pi$网络现在还有baseline，即，V网络的输出.

## Advantage A2C

Advantage A2C和A2C不一样，这里就不推导了（和baseline推导不一样，但殊途同归）。

Advantage A2C和REINFORCE网络结构一样，但是算法如下：

![image-20240703003817117](images/image-20240703003817117.png)

Advantage A2C如果再结合Multi-Step TD Target，算法如下：

![image-20240703004646002](images/image-20240703004646002.png)

此时，可以说REINFORCE算法是Advantage A2C的一种特殊情况。即，REINFORCE特指MC采样了全轨迹，而Advantage A2C可以自定义Multi-Step TD Target.

