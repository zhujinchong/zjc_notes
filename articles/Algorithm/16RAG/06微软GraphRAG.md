标题：From Local to Global: A Graph RAG Approach to Query-Focused Summarization

时间：2024.04.24

代码：[microsoft/graphrag: A modular graph-based Retrieval-Augmented Generation (RAG) system (github.com)](https://github.com/microsoft/graphrag?tab=readme-ov-file)



# 摘要

RAG问题：RAG 在针对整个文本语料库的全局问题上失败，比如“数据集中的主题是什么？”，因为这本质上是一个查询焦点摘要（QFS）任务，而不是一个明确的检索任务。

QFS：Query-Focused Summarization（查询聚焦摘要）是一种自然语言处理技术，它旨在根据用户提出的查询或问题生成摘要。这种技术的核心是理解用户的查询意图，并从大量文本中提取与查询最相关的信息，然后生成一个简洁、准确的摘要。

本文提出方案：我们提出了一种图 RAG 方法，用于在私人文本语料库上回答问题，该方法可以随着用户问题的普遍性和源文本的数量进行扩展索引。我们的方法使用一个 LLM 来构建基于图的文本索引，分为两个阶段：首先从源文档中导出实体知识图，然后为所有密切相关实体组的社区预生成摘要。给定一个问题，每个社区摘要用于生成部分回答，然后再将所有部分回答总结为最终回答呈现给用户。


总结：本文通过构建知识图谱时生成图社区摘要，以解决知识图谱在面向总结性查询时“束手无策”的问题。另外，同时结合图社区总结与子图明细可以生成更高质量的上下文

# 引言

在本文中，我们提出了一种基于全局总结的GraphRAG 方法，该方法基于 LLM 派生的知识图（图1）。与利用图索引的结构化检索和遍历可视性的相关工作相比，我们专注于在此背景下以前未探索的图的质量：它们固有的模块化，以及社区检测算法将图划分为密切相关节点的模块化社区的能力。

LLM 生成的这些社区描述的摘要提供了对底层图索引和它所代表的输入文档的完整覆盖。然后，使用一种map-reduce方法，可以实现对整个语料库的查询焦点摘要：首先使用每个社区摘要独立并并行地回答查询，然后将所有相关的部分答案总结为最终的全局答案。

![image-20240711094906160](images/06%E5%BE%AE%E8%BD%AFGraphRAG/image-20240711094906160.png)



# 2 Approach & Pipeline

我们现在解开Graph RAG 方法（图 1）和流水线的高级数据流，描述了每个步骤的关键设计参数、技术和实现细节。

## 2.1 Text Chunks文本块

原文档切分成chunks，交给LLM，用于提取图索引的各种元素：边和点的实体。

这一步潜在问题是：chunk大小怎么确定？（看下一步）

##  2.2 Element Instances元素实例

这一步的基本要求是从每个chunk中识别和提取图节点和边的实例。我们使用 LLM 来完成这一步，首先识别文本中的所有实体，包括它们的名称、类型和描述，然后识别所有明确相关实体之间的关系，包括源实体和目标实体以及它们关系的描述。这两种元素实例都以一个包含有界元组的列表输出。

这一步优化手段：

1. 提示词中加入相关领域的few-shot示例。
2. 多轮提取。

为了平衡效率和质量的需求，我们使用多轮“提取”，直到指定的最大次数，以鼓励LLM检测它可能在之前的提取轮次中遗漏的任何实体。这种方法允许我们在不降低质量（图2）或强制引入噪声的情况下使用更大的文本块大小。

个人理解：作者意思可能是建议使用2400 chunk size+多轮提取。

![image-20240711102220852](images/06%E5%BE%AE%E8%BD%AFGraphRAG/image-20240711102220852.png)



##  2.3 Element Summaries元素摘要

上一步抽取的实体、关系和描述是一种抽象的总结/摘要，这一步需要用LLM来创建独立的、有意义的、概念总结/摘要，这些总结可能是文本本身暗示但没有明确陈述的。

这一步潜在问题是：

* 上一步中LLM可能无法一致地以相同的文本格式提取对同一实体的引用，导致这一步中重复的实体元素和因此在实体图中重复的节点。

下一步可解决该问题：

* 然而，由于在接下来的步骤中将检测并总结所有密切相关的实体“社区”，并且考虑到LLM可以理解多个名称变体背后的共同实体，只要所有变体都与一组密切相关的实体有足够的连接性，我们的整体方法对这种变化是有韧性的。

总体而言，我们在可能存在噪声的图结构中对同质节点使用丰富的描述性文本，这既符合大型语言模型（LLMs）的能力，也符合全局性、以查询为中心的摘要需求。这些特性也使我们的图索引与传统的知识图谱区别开来，后者依赖于简洁一致的知识三元组（主体、谓词、对象）来进行下游推理任务。

## 2.4 Graph Communities图社区

在上一步创建的索引可以被建模为一个同质无向加权图，其中实体节点通过关系边连接，边的权重表示检测到的关系实例的归一化计数。在这样的图中，可以使用各种社区检测算法将图划分为彼此之间连接更紧密的节点的社区（例如，参见Fortunato, 2010和Jin等人, 2021的调查）。在我们的流程中，我们使用Leiden（Traag等人, 2019）是因为它能够有效地恢复大规模图的分层社区结构（见图3）。这个层次结构的每个级别提供了一个社区划分，以一种互斥的、集体穷尽的方式覆盖图的节点，从而实现了分而治之的全局摘要。

![image-20240711110522634](images/06%E5%BE%AE%E8%BD%AFGraphRAG/image-20240711110522634.png)

##  2.5 Community Summaries社区摘要

下一步是为Leiden层级中的每个社区创建类似报告的摘要，使用的方法能够适应非常大的数据集。这些摘要本身就很有用，可以帮助我们理解数据集的整体结构和含义，甚至在没有具体问题的情况下，也可以用来理解整个语料库。比如，用户可以在某个层级浏览社区摘要，寻找他们感兴趣的主题，然后通过链接查看下一层级的报告，这些报告会为每个子主题提供更多的细节。不过，在这里，我们更关注这些摘要作为基于图的索引的一部分，用来帮助回答全局性的问题。

社区摘要是通过以下方式生成的：

* 叶级社区。叶级社区（节点、边、协变量）的元素摘要被优先考虑，然后被迭代地添加到LLM上下文窗口，直到达到标记限制。优先顺序如下：对于每个社区边，按照组合源节点和目标节点度（即总体突出度）递减的顺序，添加源节点、目标节点、链接的协变量和边本身的描述。
* 更高级别的社区。如果所有元素摘要都适合于上下文窗口的标记限制，那么继续进行叶级社区的步骤，并总结社区内的所有元素摘要。否则，按照元素摘要标记数量递减的顺序对子社区进行排名，并迭代地用子社区摘要（较短）替换其关联的元素摘要（较长），直到在上下文窗口内达到适合为止。

## 2.6 Community Answers & Global Answer

给定用户查询，前一步生成的社区摘要可以用于在一个多阶段的过程中生成最终答案。社区结构的分层性质也意味着可以使用来自不同级别的社区摘要回答问题，这引发了一个问题，即在分层社区结构中的特定级别是否提供了最佳的摘要细节和范围以回答一般的理解问题（在第3节中评估）。

对于给定的社区级别，对任何用户查询的全局答案生成如下：

- 准备社区摘要。社区摘要被随机洗牌并分成预先指定的标记大小的块。这确保相关信息分布在各个块中，而不是集中（并可能丢失）在一个单一的上下文窗口中。
- 映射社区答案。并行生成中间答案，每个块一个。还要求LLM生成一个介于0-100之间的分数，指示生成的答案对回答目标问题的帮助程度。得分为0的答案将被过滤掉。
- 缩减为全局答案。中间社区答案按帮助程度分数降序排序，并迭代地添加到一个新的上下文窗口中，直到达到标记限制。这个最终上下文用于生成返回给用户的全局答案。



# 3 评估

 ## 3.1 Datasets数据集

我们选择了两个在一百万token范围内的数据集，每个数据集相当于约10本小说的文本，并代表了用户在其现实世界活动中可能遇到的语料库类型：

* Podcast transcripts：微软首席技术官与其他技术领袖之间的播客对话编制的转录。
* News articles：包括从2013年9月到2023年12月发布的各种类别的新闻文章。

##  3.2 Queries问题

之前的数据集，包括 HotPotQA（Yang 等，2018）、MultiHop-RAG（Tang 和 Yang，2024）和 MT-Bench（Zheng 等，2024），相关的问题集主要针对明确事实检索而非为了数据理解而进行总结。

为了评估 RAG 系统在更全局的理解任务中的有效性，我们采用了以活动为中心的方法来自动生成这类问题：给定数据集的简短描述，我们要求 LLM 确定 N 个潜在用户和每个用户的 N 个任务，然后对于每个（用户，任务）组合，我们要求 LLM 生成需要理解整个语料库的 N 个问题。对于我们的评估，N = 5 导致每个数据集有 125 个测试问题。表 1 显示了两个评估数据集的示例问题。
![image-20240711114805979](images/06%E5%BE%AE%E8%BD%AFGraphRAG/image-20240711114805979.png)



## 3.3 Conditions条件

我们在分析中比较了六种不同条件，包括使用四个图形社区级别（**C0**、**C1**、**C2**、**C3**）的 Graph RAG，将我们的 Map-Reduce 方法直接应用于源文本的文本总结方法（**TS**），以及一种天真的“语义搜索”RAG 方法（**SS**）：

* C0。使用根级别社区摘要（数量最少）来回答用户查询。
* C1。使用高级别社区摘要来回答查询。这些是 C0 的子社区，如果存在的话，否则向下投影到 C0 社区。
* C2。使用中间级别社区摘要来回答查询。这些是 C1 的子社区，如果存在的话，否则向下投影到 C1 社区
* C3。使用低级别社区摘要（数量最多）来回答查询。这些是 C2 的子社区，如果存在的话，否则向下投影到 C2 社区。
* TS。与第 2.6 小节中相同的方法，只是源文本（而不是社区摘要）被洗牌和分块以用于 Map-Reduce 总结阶段。
* SS。一种Naive RAG 实现，其中检索文本块并将其添加到可用上下文窗口，直到达到指定的标记限制。（这里是langchain实现的）

所有六种条件中上下文窗口的大小和用于生成答案的提示是相同的（除了对应用的上下文信息类型进行微小修改以匹配参考样式）。条件只在上下文窗口内容的创建方式上有所不同。

支持 C0-C3 条件的图形索引是仅使用我们的实体和关系提取的通用提示创建的，实体类型和少量示例针对数据领域进行了定制。图形索引过程使用了 600 个标记的上下文窗口大小，Podcast 数据集为 1 个gleaning ，News 数据集为 0 个gleaning 。



## 3.4 Metrics指标

由于缺乏黄金标准答案，我们决定采用使用 LLM 评估。四个目标指标：

- *全面性*。答案提供多少细节以涵盖问题的所有方面和细节？
- *多样性*。答案在提供不同观点和见解方面有多丰富和多样化？
- *赋能*。答案对于帮助读者理解和做出明智判断有多好？
- *直接性*。答案对问题的具体和清晰程度如何？

对于我们的评估，LLM 被提供问题、目标指标和一对答案，并被要求根据指标评估哪个答案更好，以及为什么。如果存在赢家，则返回赢家，否则如果它们在根本上相似且差异微不足道，则返回平局。为了考虑 LLM 的随机性，我们对每个比较运行五次并使用平均分数。

在两个数据集、四个指标和每个比较中的125个问题（每个重复五次并取平均）之间的（行条件）胜率百分比。每个数据集和指标的总体获胜者以粗体显示。自身获胜率未计算，但作为参考显示为预期的50%。所有GraphRAG在全面性和多样性方面均优于Naive RAG。C1-C3条件还显示出在回答全面性和多样性方面比 TS（全局文本摘要，没有图表索引）略有改进。

![image-20240711133955333](images/06%E5%BE%AE%E8%BD%AFGraphRAG/image-20240711133955333.png)

##  3.5 Configuration配置

对于任何特定任务，上下文窗口大小的影响尚不清楚，特别是对于具有128k标记上下文大小的 gpt-4-turbo 这样的模型。鉴于在更长的上下文中信息可能会“在中间丢失”（Kuratov等，2024；刘等，2023），我们希望探索变化的上下文窗口大小对我们的数据集、问题和指标组合的影响。特别是，我们的目标是确定**基线条件（SS）**的最佳上下文大小，然后统一在所有查询时的LLM使用中使用这个大小。
我们测试了四种上下文窗口大小：8k、16k、32k和64k。令人惊讶的是，测试的最小上下文窗口大小（8k）在全面性方面普遍更好（平均胜率为58.1%），而在多样性方面表现与较大上下文大小相当（平均胜率为52.4%），以及赋权方面（平均胜率为51.3%）。鉴于我们更偏好更全面和多样化的答案，因此我们在最终评估中使用了固定的8k标记上下文窗口大小。

##  3.6 Results

索引过程导致播客数据集的图表包含8564个节点和20691条边，新闻数据集的更大图表包含15754个节点和19520条边。表3显示了每个图表社区层次结构不同级别的社区摘要数量。

![img](images/06%E5%BE%AE%E8%BD%AFGraphRAG/27f5d9826b5a4bd5bffb1675d0cc7251.png)

Global approaches vs. naiveRAG.

如图4所示，全局方法在播客转录中的全面性胜率介于72-83%，新闻文章中为72-80%，而多样性胜率分别为75-82%和62-71%。我们使用直接性作为有效性测试也取得了预期的结果，即Naive RAG在所有比较中产生最直接的回答。

Community summaries vs. source texts. 

使用GraphRAG将社区摘要与源文本进行比较时，社区摘要通常在回答全面性和多样性方面提供了一种小但一致的改进，除了根级摘要。播客数据集中的中级摘要和新闻数据集中的低级社区摘要分别实现了57%和64%的全面性胜率。多样性胜率为播客中级摘要为57%，新闻低级社区摘要为60%。表3还说明了与源文本摘要相比，图表 RAG的可扩展性优势：对于低级社区摘要（C3），图表 RAG需要少26-33%的上下文标记，而对于根级社区摘要（C0），它需要超过97%的更少标记。与其他全局方法相比，根级图表 RAG 在保持全面性（72%胜率）和多样性（62%胜率）优势的同时，为表征感知活动的迭代问答提供了一种高效的方法。

Empowerment.

赋能比较显示，全局方法与 Naive RAG（**SS**）以及GraphRAG方法与源文本摘要（**TS**）之间的结果各有千秋。用于分析此度量的临时LLM使用表明，提供具体示例、引用和引文的能力被认为是帮助用户达到明智理解的关键。调整元素提取提示可能有助于在GraphRAG索引中保留更多这些细节。

# 结论

我们提出了一种全局的Graph RAG 方法，结合知识图生成、检索增强生成（RAG）和面向查询的摘要（QFS），以支持对整个文本语料库的人类理解。初步评估显示，与Naive RAG 基线相比，我们的方法在回答的全面性和多样性方面取得了显著改进，与使用映射-减少源文本摘要的全局但无图形方法进行了有利的比较。对于需要在同一数据集上进行许多全局查询的情况，实体为基础的图形索引中根级社区的摘要提供了一个数据索引，既优于Naive RAG，又在Token成本的一小部分上实现了与其他全局方法的竞争性表现。

