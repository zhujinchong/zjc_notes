两个方向：

1. 频率派：统计机器学习
2. 贝叶斯派：概率图模型

 

必读书籍：

1. 李航，统计机器学习
2. 周志华：西瓜书
3. PRML，模式识别与机器学习
4. MLAPP，Machine Learning：A Probabilistic Perspective
5. ESL，The Elements of Statistical Learning
6. Deep Learning



# 	机器学习和深度学习关系



![在这里插入图片描述](images/20200510174301321.jpg)



# Introduction

对概率的诠释有两大学派，一种是频率派另一种是贝叶斯派。$N$ 个样本，$p$ 维的数据集表示： $$ X_{N\times p}=(x_{1},x_{2},\cdots,x_{N})^{T} $$ ，其中每个观测都是由 $p(x|\theta)$ 生成的。

$$= \begin{bmatrix} x_{11}  & \cdots &  x_{1p}     \\ \vdots & \ddots & \vdots \\   x_{N1}    & \cdots & x_{Np} \end{bmatrix}$$

## 频率派的观点

$p(x|\theta)$中的 $\theta$ 是一个常量。对于 $N$ 个观测来说观测集的概率为 $p(X|\theta) = \prod p(x_{i}|\theta)$ 。为了求 $\theta$ 的大小，我们采用最大对数似然MLE的方法：

$$ \theta_{MLE}=\mathop{argmax}\limits _{\theta}\log p(X|\theta)\mathop{=}\limits _{iid}\mathop{argmax}\limits _{\theta}\sum \log p(x_{i}|\theta) $$

其中iid表示独立同分布；为了简化计算，加log。

频率派认为问题是优化问题：如回归最小二乘、梯度下降；SVM；EM

## 贝叶斯派的观点

贝叶斯派认为 $p(x|\theta)$ 中的 $\theta$ 不是一个常量。这个 $\theta$ 满足一个预设的先验的分布 $\theta\sim p(\theta)$ 。于是根据贝叶斯定理依赖观测集参数的后验可以写成：

$$ p(\theta|X)=\frac{p(X|\theta)\cdot p(\theta)}{p(X)}=\frac{p(X|\theta)\cdot p(\theta)}{\int\limits _{\theta}p(X|\theta)\cdot p(\theta)d\theta} $$ 为了求 $\theta$ 的值，我们要最大化这个参数后验，最大后验概率估计MAP：

$$ \theta_{MAP}=\mathop{argmax}\limits _{\theta}p(\theta|X)=\mathop{argmax}\limits _{\theta}p(X|\theta)\cdot p(\theta) $$， 其中第二个等号是由于分母和 $\theta$ 没有关系。

得到了参数的后验分布后，我们可以将这个分布用于贝叶斯预测： $$ p(x_{new}|X)=\int_{\theta}p(x_{new}, \theta | X) d\theta=\int\limits_{\theta}p(x_{new}|\theta) p(\theta|X)d\theta $$ ，其中将$\theta$作为中间桥梁。

贝叶斯派认为问题是积分问题，求后验是推断问题，之后再进行决策（预测）：

## 小结

频率派和贝叶斯派分别给出了一系列的机器学习算法。频率派的观点导出了一系列的统计机器学习算法，而贝叶斯派导出了概率图理论。在应用频率派的 MLE 方法时最优化理论占有重要地位（损失函数），而贝叶斯派的算法无论是后验概率的建模还是应用这个后验进行推断时积分占有重要地位（MCMC采样求积分）。

频率派：1、设计模型；2、损失函数；3、优化方法（梯度下降等）